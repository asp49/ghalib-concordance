{
 "metadata": {
  "name": "",
  "signature": "sha256:f5fc4da6bc74f071454b159227c34ea237a11060a7ffa5f00f3dae021d0ce5ab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Ghalib Concordance Generator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Description\n",
      "\n",
      "This notebook contains code to generator a concordance for the muravvaj divaan of ghalib"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verses = {} # dictionary of verses, e.g. 001.01.0='naqsh faryaadii..'\n",
      "tokens = {} # dictionary of tokens where key is verses+.xx, e.g. 001.01.0.01 = 'naqsh'\n",
      "unique_tokens = {} # dictionary of tokens where value is their count\n",
      "lemmas = {} # dictionary of tokens where value is a list of their lemmas\n",
      "unique_lemmas = [] # dictionary of unique lemmas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def load_verses(inputfile='input/verses.csv'):\n",
      "    '''\n",
      "    Loads verses from CSV data file\n",
      "    inputfile: name of csv file\n",
      "    returns: verses where verses['ggg.vv.l']=token; where ggg=ghazal #; vv=verse number;l=line number\n",
      "    '''\n",
      "\n",
      "    import csv\n",
      "    verses = {}\n",
      "    with open(inputfile,'r') as csvfile:\n",
      "        versereader = csv.reader(csvfile)\n",
      "        for row in versereader:\n",
      "            (verse_id, input_string, real_scan) = row # \n",
      "            if not 'x' in verse_id: # skip ones with x (not in muravvaj divan)\n",
      "                verses[verse_id] = input_string.strip()\n",
      "    return verses\n",
      "\n",
      "def get_tokens(verses):\n",
      "    '''\n",
      "    Identifies tokens in verses\n",
      "    verses: verses\n",
      "    returns: tokens, where tokens['ggg.vv.l.tt']=token {tt = token # on line starting  at zero}\n",
      "    '''\n",
      "    tokens = {}\n",
      "    for k in verses.keys():\n",
      "        v_tokens = verses[k].split(' ')\n",
      "        for id,t in enumerate(v_tokens):\n",
      "            token_id = k+'.'+str(id).zfill(2)\n",
      "            tokens[token_id] = t\n",
      "    return tokens\n",
      "\n",
      "def locate_token(token):\n",
      "    '''\n",
      "    Finds locations of token\n",
      "    token: string \n",
      "    Input: token (string)\n",
      "    returns: a list of locations, e.g. [001.01.0.01]\n",
      "    '''\n",
      "    assert tokens\n",
      "    return [k  for k,v in tokens.iteritems() if v==token]\n",
      "\n",
      "def match_tokens(match_string):\n",
      "    '''\n",
      "    Finds tokens matching a pattern (from start)\n",
      "    match_string: regular expression string (assumes ^,e.g. 'naq')\n",
      "    returns: a list of tokens,e.g. ['naqsh']\n",
      "    '''\n",
      "    import re\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.match(match_string,k)]\n",
      "\n",
      "def search_tokens(match_string):\n",
      "    '''\n",
      "    Searches for tokens matching a pattern (anywhere in it)\n",
      "    match_string: regular expression of string\n",
      "    Input: regular expression string (e.g. 'aqsh'\n",
      "    returns: a list of tokens, e.g. ['naqsh']\n",
      "    '''\n",
      "    import re\n",
      "    assert unique_tokens\n",
      "    return [k  for k in unique_tokens.keys() if re.search(match_string,k)]\n",
      "\n",
      "def get_unique_tokens(tokens):\n",
      "    '''\n",
      "    Finds unique tokens\n",
      "    tokens: a dictionary of tokens at locations, e.g. tokens['001.01.0.00']='naqsh'\n",
      "    returns: a dictionary of unique tokens and their count, unique_tokens['token']=1\n",
      "    '''\n",
      "    unique = {}\n",
      "#    print type(tokens)\n",
      "    for k,t in tokens.iteritems():\n",
      "\n",
      "        if not t in unique: \n",
      "            unique[t]=0\n",
      "            \n",
      "        unique[t]+=1\n",
      "    return unique\n",
      "\n",
      "\n",
      "def get_lemmas(unique_tokens):\n",
      "    '''\n",
      "    Generate lemmas of tokens\n",
      "    unique_tokens: dictionary of unique tokens\n",
      "    returns: lemmas[original_token]=['lemma1','lemma2']\n",
      "    '''\n",
      "    lemmas = {}\n",
      "\n",
      "    for t in unique_tokens.keys():\n",
      "        lemma = t\n",
      "\n",
      "        if re.search(\"[-']haa$\",t): \n",
      "            lemma = t[:-4] # remove Persian plural ['-]haa ending\n",
      "        if re.search(\"-e$\",t):\n",
      "            lemma = t[:-2] # remove izaafat ending '-e'\n",
      "#            print lemma\n",
      "        t_lemmas = [lemma]\n",
      "        if re.search('-o-',lemma):\n",
      "            nouns = lemma.split('-o-')\n",
      "            t_lemmas = t_lemmas + nouns\n",
      "            \n",
      "        lemmas[t]=t_lemmas\n",
      "    return lemmas\n",
      "\n",
      "def get_unique_lemmas(lemmas):\n",
      "    '''\n",
      "    Generates unique lemma forms\n",
      "    lemmas: dictionary keyed by tokens containing lists of lemma, e.g. lemmas['rang-o-buu']=['rang','buu','rang-o-buu']\n",
      "    returns: unique_lemmas as unique_lemmas['lemma']=count\n",
      "    '''\n",
      "    unique_lemmas = []\n",
      "    for t,t_lemmas in lemmas.iteritems():\n",
      "        for lemma in t_lemmas:\n",
      "            if not lemma in unique_lemmas:\n",
      "                unique_lemmas.append(lemma)\n",
      "#            else:\n",
      "#                unique_lemmas[lemma].append(t)\n",
      "    return unique_lemmas\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Set Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verses = load_verses()\n",
      "tokens = get_tokens(verses)\n",
      "unique_tokens = get_unique_tokens(tokens)\n",
      "lemmas = get_lemmas(unique_tokens)\n",
      "unique_lemmas = get_unique_lemmas(lemmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#okay, to_check = get_okay_and_to_check(unique_lemmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Write Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('output/okay.txt','w') as f:\n",
      "    f.write('\\n'.join(okay))\n",
      "with open('output/have_to_check.txt','w') as f:\n",
      "    f.write('\\n'.join(to_check))\n",
      "with open('output/tokenlemmas.csv','w') as f:\n",
      "    for t in sorted(unique_tokens.keys()):\n",
      "        line  = \",\" # good or bad\n",
      "        line += t+\",\" #token\n",
      "        line += '|'.join(lemmas[t]) # possible lemma of token\n",
      "        line += \"\\n\" \n",
      "        f.write(line)\n",
      "        \n",
      "                    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}